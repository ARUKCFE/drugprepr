---
title: "drugprepCPRD package design document"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{drugprepCPRD package design document}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The purpose of this document is to plan out the design of the package.
(The academic scientific motivation is left for another vignette.)

## Basic idea

We start with a dataset that looks a little bit like this.

```{r setup}
library(drugprepCPRD)
knitr::kable(dataset)
```

Where values are **implausible** (larger than the reference maximum, smaller than the reference minimum) or **missing** (i.e. `NA`), we have to choose how to replace them.
The main methods are

1. Do nothing (identity)
2. Mark value as `NA`
3. Impute with the mean of plausible non-missing values
4. Impute with the mode of plausible non-missing values
5. Impute with the median of plausible non-missing values
6. Last (plausible, non-missing) observation carried forward
7. Next (plausible, non-missing) observation carried backward

and for the latter five options, there is also the choice of whether to compute these imputed values from

1. the individual patient
2. the patient's GP practice
3. the entire population.

When applied to both the variables corresponding to numeric daily dose and the quantity of dose, for both implausible and missing values, we obtain a labyrinthine garden of forking paths with
\[7^2 \times 3^2 = 441\]
different possible ways of processing the data, and that's just for those two variables.
More combinations may emerge for handling dosage start and end times or other factors.

We'd therefore like a way of exploring some or all of these possibilities to see what influence the different decisions have on an analysis.
Ideally, we can generate a dataset corresponding to each decision and fit a model (or compute a summary, say) for each one.

## Example analysis

Before wrapping things into a package and trying to optimize, let's consider a case study and how we might approach the problem _without_ the benefit of a dedicated set of functions, so it becomes clear where the tedious or complicated parts are.
Then we can wrap things up as we go.

The atomic element of the package should be an `impute` function, which takes three principal inputs:

1. the variable of interest
2. indices to replace (e.g. which values are 'implausible' or missing)
3. a function to impute (one of the 7 above)
4. which group to compute the imputed value from (one of the 3 above).

Concretely, we might use **tidyverse** syntax like

```{r, eval=FALSE}
dataset %>%
  group_by(grp)
  mutate(ndd = replace(ndd, ids_to_replace, fun(ndd)))
```

where `grp` may take the values `patid` (individual), `pracid` (practice) or `NULL` (population); `ids_to_replace` is either `is.na(ndd)` or else a logical vector indicating which elements are implausible; `fun` is a summary function such as `mean`, `median`, `identity`; and `ndd` here may be swapped for the variable of interest, such as `qty` or some other generic symbol.

We might also consider a **data.table** implementation with similar notation:

```{r, eval=FALSE}
dataset[ids_to_replace, ndd := fun(ndd), by = grp]
```

Everything beyond this is simply writing wrappers to handle multiple variables or decisions and pass these arguments to such an expression, as well as to collat the outputs into a dataset for fitting and comparing models.
